{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH/+mb17IBUjQQnOO19nYp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praffuln/agentic-ai/blob/master/langgraph_agentic_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi8aB8amSB87",
        "outputId": "0c2e9ad4-20f7-41e9-b727-473013fcb492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ],
      "source": [
        "# prompt: print hello world\n",
        "\n",
        "print(\"hello world\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai tiktoken chromadb langgraph langchain langchain_community langchainhub ipykernel langchain_groq sentence_transformers\n"
      ],
      "metadata": {
        "id": "XUPvgZUquE8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, Literal, Sequence, TypedDict\n",
        "from langchain import hub\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.prebuilt import ToolNode"
      ],
      "metadata": {
        "id": "lKjLajjk3T_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Keys For GOOGLE_API_KEY\n"
      ],
      "metadata": {
        "id": "YPFF-ppevar4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assigning value to variable\n",
        "GEMINI_API_KEY=''\n",
        "SERPER_API_KEY = ''\n",
        "GROQ_API_KEY = ''\n"
      ],
      "metadata": {
        "id": "LbM3-zGMvdqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup environments\n"
      ],
      "metadata": {
        "id": "n0zcWVGYvr1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['SERPER_API_KEY'] = SERPER_API_KEY\n",
        "os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
        "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LgoK_XsEvygs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setup LLM"
      ],
      "metadata": {
        "id": "hDjCG2BM4415"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\"Explain how AI works\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "zfuv4uE3Dj2A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "91129462-a01d-4633-ad41-7e19a7e6a800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI works by mimicking human intelligence processes through machines, particularly computer systems.  There's no single \"how it works,\" as different AI approaches employ different techniques. However, several core concepts are common:\n",
            "\n",
            "**1. Data as the Foundation:** AI systems learn from data.  The more data they're trained on, the better they generally perform. This data can be structured (like tables in a database) or unstructured (like text, images, or audio).\n",
            "\n",
            "**2. Algorithms as the Engine:** Algorithms are sets of rules and statistical techniques that AI uses to analyze data, identify patterns, and make predictions or decisions.  These algorithms are the \"brains\" of the AI system, dictating how it processes information.  Different algorithms are suited for different tasks.\n",
            "\n",
            "**3. Learning from Data:** This is the crucial aspect. AI systems don't just follow pre-programmed instructions; they learn from the data they are fed.  This learning can take several forms:\n",
            "\n",
            "* **Supervised Learning:** The AI is trained on a labeled dataset, meaning the data includes both input and the desired output.  The algorithm learns to map inputs to outputs.  Example: training an image recognition system by showing it thousands of images labeled \"cat\" or \"dog.\"\n",
            "\n",
            "* **Unsupervised Learning:** The AI is trained on an unlabeled dataset.  The algorithm tries to find patterns and structures in the data without explicit guidance.  Example: clustering customers into different groups based on their purchasing behavior.\n",
            "\n",
            "* **Reinforcement Learning:** The AI learns through trial and error by interacting with an environment.  It receives rewards for desirable actions and penalties for undesirable actions.  Example: training a robot to navigate a maze by rewarding it for reaching the goal and penalizing it for hitting walls.\n",
            "\n",
            "* **Deep Learning:** A subfield of machine learning that uses artificial neural networks with multiple layers (hence \"deep\"). These networks can learn complex patterns and representations from data, often achieving state-of-the-art results in tasks like image recognition and natural language processing.\n",
            "\n",
            "**4. Models as the Representation:**  As AI learns from data, it creates a *model* – a mathematical representation of the patterns and relationships it has discovered. This model is then used to make predictions or decisions on new, unseen data.\n",
            "\n",
            "**5. Evaluation and Refinement:** AI systems are constantly evaluated to measure their performance.  Metrics like accuracy, precision, and recall are used to assess how well the system is working.  Based on this evaluation, the system's algorithms, models, or training data may be refined to improve its performance.\n",
            "\n",
            "**In short:** AI systems work by ingesting large amounts of data, using algorithms to identify patterns within that data, creating a model that represents those patterns, and then using that model to make predictions or decisions on new data. The process is iterative, with constant evaluation and refinement to improve performance.  Different techniques, like supervised, unsupervised, and reinforcement learning, and deep learning, are used depending on the specific task and the nature of the available data.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from sentence_transformers import SentenceTransformer # Import SentenceTransformer instead of GroqEmbedding\n",
        "\n",
        "\n",
        "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
        "\n",
        "llm.invoke(\"tell me something about ai\")\n",
        "\n",
        "# Now use SentenceTransformer for embedding\n",
        "embedding_model = SentenceTransformer('all-mpnet-base-v2')  # Choose a suitable pre-trained model\n",
        "embedding = embedding_model.encode(\"tell me something about ai\")\n",
        "\n",
        "print(embedding) # Print the embedding to see the result"
      ],
      "metadata": {
        "id": "6DmQlQ0FViCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "fYT_mkVMKDrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls= [\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "      \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"]\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "\n",
        "docs[0][0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6S3v7jPKF7u",
        "outputId": "925db5aa-dbd8-4d20-af2b-9582b91595eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              " 'title': \"LLM Powered Autonomous Agents | Lil'Log\",\n",
              " 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.',\n",
              " 'language': 'en'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=5)\n",
        "doc_splits=text_splitter.split_documents(docs_list)\n",
        "doc_splits"
      ],
      "metadata": {
        "id": "L0nILKQtMm7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "\n",
        "\n",
        "vectorstore=Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chrome\",\n",
        "    embedding=embedding_model\n",
        ")\n"
      ],
      "metadata": {
        "id": "wCTnL03wOGVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "QWWBnrbyPpaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### convert retriever to tool"
      ],
      "metadata": {
        "id": "66-IFDTlqrgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_blog_posts\",\n",
        "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.You are a specialized assistant. Use the 'retriever_tool' **only** when the query explicitly relates to LangChain blog data. For all other queries, respond directly without using any tool. For simple queries like 'hi', 'hello', or 'how are you', provide a normal response.\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "YLf2hjNEqx55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools=[retriever_tool]"
      ],
      "metadata": {
        "id": "1EsabF4EsEYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fuctions & Graph"
      ],
      "metadata": {
        "id": "ZkOKdhL4ANmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]"
      ],
      "metadata": {
        "id": "24X3jfqbUJ6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_assistant(state):\n",
        "    print(\"---CALL AGENT---\")\n",
        "    messages = state['messages']\n",
        "    print(f\"this is my message: {messages}\")\n",
        "\n",
        "    llm_with_tool = llm.bind_tools(tools)\n",
        "    response = llm_with_tool.invoke(messages)\n",
        "    #response=handle_query(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "\n",
        "    # print(\"---CALL AGENT---\")\n",
        "    # messages = state['messages']\n",
        "    # print(f\"this is my message: {messages}\")\n",
        "\n",
        "    # if len(messages)>1:\n",
        "    #     response=llm.invoke(messages[-1].content)\n",
        "    #     return {\"messages\": [response]}\n",
        "    # else:\n",
        "    #     llm_with_tool = llm.bind_tools(tools)\n",
        "    #     response = llm_with_tool.invoke(messages)\n",
        "    #     #response=handle_query(messages)\n",
        "    #     return {\"messages\": [response]}\n"
      ],
      "metadata": {
        "id": "5RUsp2p0ARbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(state):\n",
        "  pass"
      ],
      "metadata": {
        "id": "0peOMOp2AVUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import  HumanMessage\n",
        "def rewrite(state:AgentState):\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "\n",
        "    print(f\"here is message from rewrite: {messages}\")\n",
        "\n",
        "    message = [HumanMessage(content=f\"\"\"Look at the input and try to reason about the underlying semantic intent or meaning.\n",
        "                    Here is the initial question: {question}\n",
        "                    Formulate an improved question: \"\"\")\n",
        "       ]\n",
        "    response = llm.invoke(message)\n",
        "    return {\"messages\": [response]}\n"
      ],
      "metadata": {
        "id": "sLYBYeFNAg8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state:AgentState):\n",
        "    print(\"---GENERATE---\")\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    print(f\"here is message from generate: {messages}\")\n",
        "\n",
        "    question = messages[0].content\n",
        "    last_message = messages[-1]\n",
        "    docs = last_message.content\n",
        "\n",
        "    prompt = \"\"\"\n",
        "                You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
        "                If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "                Question: {question}\n",
        "                Context: {context}\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "\n",
        "    rag_chain = prompt | llm\n",
        "\n",
        "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "    print(f\"this is my response:{response}\")\n",
        "\n",
        "    return {\"messages\": [response]}\n"
      ],
      "metadata": {
        "id": "WywPV4WiAktk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow=StateGraph(AgentState)\n",
        "workflow.add_node(\"My_AI_Assistant\",ai_assistant)\n",
        "retrieve=ToolNode([retriever_tool])   ### way to add retriever_tool under node\n",
        "workflow.add_node(\"retriever\", retrieve)\n",
        "workflow.add_node(\"Query_Rewriter\", rewrite)\n",
        "workflow.add_node(\"Output_Generator\", generate)\n"
      ],
      "metadata": {
        "id": "josu2KMNAsq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_edge(START, \"My_AI_Assistant\")\n",
        "workflow.add_edge(\"My_AI_Assistant\", \"Vector_Retriever\")\n",
        "# workflow.add_conditional_edges()"
      ],
      "metadata": {
        "id": "0z-oLiDQBetq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "BMKy1NHDEVBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke({\"messages\": [HumanMessage(content=\"tell me something about ai\")]})"
      ],
      "metadata": {
        "id": "GktjJiiJEc8K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}